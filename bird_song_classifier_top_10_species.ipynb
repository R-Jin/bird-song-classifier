{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R-Jin/bird-song-classifier/blob/main/bird_song_classifier_top_10_species.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset location"
      ],
      "metadata": {
        "id": "KIPSI43ZErQo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVsGjuCmEN5o",
        "outputId": "e66ecb2c-eb83-4164-c2cb-8347910d2eae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setup target directory\n",
        "target_dir = \"/content/drive/MyDrive/birdclef-2025\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "JKbn493nMJjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "J0n1yhBPLzXP"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "r8BJltcyOd7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "SAMPLE_RATE = 32000\n",
        "HOP_LENGTH = 512"
      ],
      "metadata": {
        "id": "bep40MqxObKn"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some unwanted song type labels... may use later (Filter out unwanted labels, e.g. \"frog\", and combine labels with similar names)\n"
      ],
      "metadata": {
        "id": "zFMK2DyeS3t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unwanted_labels = [\n",
        "    'uncertain', 'frog', 'frogs', 'car', 'plane', 'wind', 'river', 'surf',\n",
        "    'motor', 'insect', 'noise', 'background', 'voices', 'human', 'dog',\n",
        "    'perro', 'cow', 'latido', 'engine', 'traffic', 'urban', 'música',\n",
        "    'music', 'unknown', 'no idea', '?', 'nao', 'não', 'environmental',\n",
        "    'ambient', 'dogs', 'bird in h', 'laughing', 'hits', 'chatter', 'mechanical', 'imitation',\n",
        "    'river in background', 'human voices', 'car noises', 'frogs and insects'\n",
        "]\n",
        "\n",
        "canonical_labels = {\n",
        "    'song': ['song', 'songs', 'full song', 'complete song', 'alternate song', 'display song', 'morning song', 'evening song', 'day song', 'dawn song', 'vesper song', 'nuptial song', 'predawn song'],\n",
        "    'call': ['call', 'calls', 'typical call', 'short call', 'long call', 'flight call', 'begging call', 'alarm call', 'chase call', 'contact call', 'feeding call', 'distress call', 'display call', 'excited call', 'juvenile call'],\n",
        "    'alarm call': ['alarm call', 'alarm calls', 'alarm', 'alarm whistle', 'alarm call?', 'alarm call in flight'],\n",
        "    'flight call': ['flight call', 'calls in flight', 'call in flight', 'flight calls', 'take-off calls', 'take-off call'],\n",
        "    'duet': ['duet', 'duet song', 'duet call', 'couple duet', 'song and duet', 'duetting'],\n",
        "    'drum': ['drum', 'drumming', 'bill snap', 'bill snapping', 'wing beat', 'wing flaps', 'wing noise', 'wing flutter', 'wing flutters', 'wing sounds', 'wing whirr', 'wing whirrs', 'wing snaps', 'bill/wing snapping'],\n",
        "    'subsong': ['subsong', 'subsong?', '\"subsong\"'],\n",
        "    'trill': ['trill', 'trilled call', 'trill call', 'cooing trill'],\n",
        "    'display': ['display', 'display song', 'display call', 'song during display flight', 'display song & instrumental wings noise'],\n",
        "}\n"
      ],
      "metadata": {
        "id": "TDuSAy7nJndg"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read and clean data"
      ],
      "metadata": {
        "id": "DviycMaJXxaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only necessary for cleaning\n",
        "#unwanted_labels = ['aberrant', 'perched', 'bill snaps', 'rattle', 'scold', 'snap', 'chatter', 'mechanical sounds', 'wind', 'lekking', 'lek', 'grunts', 'tapping', 'mechanical', 'voces humanas', 'immature', 'roll-snap', 'bill/wing snapping', 'estridulación', 'frogs', 'bill rattle']\n",
        "\n",
        "label_map = {\n",
        "    'song' : 'song',\n",
        "    '' : 'unknown', #?????\n",
        "    'call' : 'call',\n",
        "    'flight call' : 'flight call',\n",
        "    'alarm call' : 'alarm call',\n",
        "    'duet' : 'duet',\n",
        "    'calls' : 'call',\n",
        "    'dawn song' : 'song', # ?\n",
        "    'uncertain' : 'unknown', #?????\n",
        "    'begging call' : 'begging call',\n",
        "    'drumming' : 'drumming',\n",
        "    'subsong' : 'subsong',\n",
        "    'canto' : 'song',\n",
        "    'flight calls' : 'flight call',\n",
        "    'nocturnal flight call' : 'flight call', # ?\n",
        "    # aberrant?\n",
        "    'chamado' : 'call',\n",
        "    'advertisement call' : 'call', # call?\n",
        "    'hatchling or nestling' : 'hatchling or nestling',\n",
        "    'interaction calls' : 'call', # call?\n",
        "    'songs' : 'song',\n",
        "    '?' : 'unknown',\n",
        "    'song?' : 'song',\n",
        "    'chorus' : 'song', # ?\n",
        "    'alarm calls' : 'alarm call',\n",
        "    'wing sound' : 'wing sound',\n",
        "    'long song' : 'song',\n",
        "    'imitation' : 'imitation',\n",
        "    'wing sounds' : 'wing sound',\n",
        "    'wing beats' : 'wing sound',\n",
        "    'wing noise' : 'wing sound',\n",
        "    # lekking?\n",
        "    'display song' : 'song', # ?\n",
        "    # perched?\n",
        "    'excited calls' : 'call', #?\n",
        "    # bill snaps?\n",
        "    'morning song' : 'song', # ?\n",
        "    'chase calls' : 'call', # ?\n",
        "    'wing whirrs' : 'wing sound',\n",
        "    # rattle?\n",
        "    'alarm' : 'alarm call',\n",
        "    'drum' : 'drumming',\n",
        "    'wing snaps' : 'wing sound',\n",
        "    'dueto' : 'duet',\n",
        "    'short song' : 'song',\n",
        "    # scold?\n",
        "    'call?' : 'call',\n",
        "    'song in flight' : 'song', # ?\n",
        "    'song in duet' : 'duet',\n",
        "    # snap?\n",
        "    'mimicry/imitation' : 'imitation',\n",
        "    'flight song': 'song', # ?\n",
        "    'agitated calls' : 'call', # ?\n",
        "    'interaction call' : 'call',\n",
        "    'alternate song' : 'song',\n",
        "    'take-off and flight calls' : 'flight call',\n",
        "    'calls at roost' : 'call', # ?\n",
        "    # lek ?\n",
        "    'half song' : 'song',\n",
        "    'couple song' : 'duet', #????\n",
        "    # chatter?\n",
        "    'short call' : 'call',\n",
        "    'typical call' : 'call',\n",
        "    # mechanical sounds?\n",
        "    'mimicking' : 'imitation',\n",
        "    'wing beat' : 'wing sound',\n",
        "    'bater de asas' : 'wing sound',\n",
        "    'song a' : 'song',\n",
        "    'agitated song' : 'song', #????\n",
        "    'duet song' : 'duet',\n",
        "    'batido de asas' : 'wing sound',\n",
        "    'wing flutter' : 'wing sound',\n",
        "    # wind??\n",
        "    'immature calls' : 'call',\n",
        "    'daytime song' : 'song', # ?\n",
        "    '\"call' : 'call',\n",
        "    'excited song' : 'song', # ?\n",
        "    'song 2' : 'song',\n",
        "    'wing flaps' : 'wing sound',\n",
        "    'aggressive call' : 'call',\n",
        "    'territorial call' : 'call',\n",
        "    'feeding call' : 'call',\n",
        "    'take-off calls' : 'flight call', # ?????\n",
        "    'contact calls' : 'call',\n",
        "    'song during display flight' : 'song',\n",
        "    'calls on perch' : 'call',\n",
        "    'perched call' : 'call',\n",
        "    # grunts ?\n",
        "    'daytime calls' : 'call',\n",
        "    'daytime call' : 'call',\n",
        "    'dawn duet' : 'duet',\n",
        "    'take-off call' : 'flight call', # ?????\n",
        "    'calls (2 types)' : 'call',\n",
        "    'perched calls' : 'call',\n",
        "    # tapping ?\n",
        "    # mechanical ?\n",
        "    # voces humanas ?\n",
        "    'agitated call' : 'call',\n",
        "    'single song' : 'song',\n",
        "    'distress call of a bird being handled.' : 'call',\n",
        "    'nuptial song' : 'song',\n",
        "    'begging calls' : 'begging call',\n",
        "    # immature ?\n",
        "    'vesper song' : 'song',\n",
        "    'song (variation)' : 'song',\n",
        "    # roll-snap ?\n",
        "    'call in flight' : 'call',\n",
        "    'chase call' : 'call',\n",
        "    'canto de voo' : 'song',\n",
        "    # estridulación ?\n",
        "    # frogs ?\n",
        "    # bill rattle ?\n",
        "    'couple duet' : 'duet',\n",
        "    'rattle calls' : 'call',\n",
        "\n",
        "    # song while displaying.....\n",
        " }"
      ],
      "metadata": {
        "id": "4WhH1KHpXwFJ"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/birdclef-2025/train.csv\")\n",
        "\n",
        "def clean_label(label):\n",
        "  results = re.findall(r\"'([^']*)'\", label)\n",
        "  for result in results:\n",
        "    result = result.lower()\n",
        "\n",
        "    if pd.isna(label):\n",
        "        return None\n",
        "    label = str(label).lower().strip()\n",
        "    return label_map.get(label, None)\n",
        "\n",
        "def clean_labels(df): # Pass df as an argument\n",
        "  cleaned = []\n",
        "  for song_types in df['type']: # Iterate over df['type']\n",
        "    results = re.findall(r\"'([^']*)'\", song_types)\n",
        "    labels = []\n",
        "    for result in results:\n",
        "      # Convert to lowercase for consistency\n",
        "      result = result.lower()\n",
        "      #result = label_map.get(result, result)\n",
        "      result = label_map.get(result, None)\n",
        "\n",
        "      if result is not None:\n",
        "        if result not in labels:\n",
        "          labels.append(result)\n",
        "\n",
        "    if len(labels) == 0:\n",
        "      labels = ['unknown']\n",
        "\n",
        "    cleaned.append(labels)\n",
        "  return cleaned # Return the cleaned list\n",
        "print(len(train_df))\n",
        "train_df['type'] = clean_labels(train_df)\n",
        "print(len(train_df))"
      ],
      "metadata": {
        "id": "4A0gBwe-y7hy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078dfb1d-cddf-4f82-e933-c499dd494cb3"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28564\n",
            "28564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.dropna(subset=['latitude', 'longitude'])"
      ],
      "metadata": {
        "id": "pmaI9q1-y8Qv"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N_SPECIES = 10\n",
        "sample_size = 1000\n",
        "samples_per_class = sample_size // TOP_N_SPECIES\n",
        "\n",
        "# Get top-N species by frequency\n",
        "top_species = train_df['primary_label'].value_counts().nlargest(TOP_N_SPECIES).index.tolist()\n",
        "\n",
        "# Filter DataFrame\n",
        "train_df = train_df[train_df['primary_label'].isin(top_species)].reset_index(drop=True)\n",
        "\n",
        "train_df = train_df.groupby('primary_label', group_keys=False).apply(lambda x: x.sample(n=samples_per_class)).reset_index(drop=True)\n",
        "train_df = train_df.sample(sample_size).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "M8wjCVjOxM4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3f4a7f-8cec-418e-ca26-edd9306db38e"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-128-45109530633f>:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  train_df = train_df.groupby('primary_label', group_keys=False).apply(lambda x: x.sample(n=samples_per_class)).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate number of unique species and song types"
      ],
      "metadata": {
        "id": "0JMpHeHTOfSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "# Turn secondary label to list instead of string\n",
        "\n",
        "unique_song_types = set()\n",
        "unique_bird_species = set()\n",
        "\n",
        "for bird_species in train_df['primary_label']:\n",
        "  unique_bird_species.add(bird_species)\n",
        "\n",
        "for song_types in train_df['type']:\n",
        "  for song_type in song_types:\n",
        "    unique_song_types.add(song_type)\n",
        "\n",
        "print(f\"Number of unique bird species: {len(unique_bird_species)}\" )\n",
        "print(f\"Number of unique song types: {len(unique_song_types)}\")\n",
        "print(unique_bird_species)\n",
        "print(unique_song_types)\n",
        "print(f\"Number of samples: {len(train_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7v_mUrfjXpW",
        "outputId": "14624f70-d843-4df0-82f2-b7aa4c2ed63e"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique bird species: 10\n",
            "Number of unique song types: 10\n",
            "{'trokin', 'banana', 'yeofly1', 'roahaw', 'bobfly1', 'wbwwre1', 'grekis', 'compau', 'whtdov', 'socfly1'}\n",
            "{'unknown', 'begging call', 'wing sound', 'song', 'call', 'subsong', 'duet', 'drumming', 'alarm call', 'flight call'}\n",
            "Number of samples: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remap species and song type to index"
      ],
      "metadata": {
        "id": "CV3ZBoB2E7_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maps of song types and bird species to labels\n",
        "song_type_to_label = {song_type: i for i, song_type in enumerate(unique_song_types)}\n",
        "bird_species_to_label = {bird_species: i for i, bird_species in enumerate(unique_bird_species)}"
      ],
      "metadata": {
        "id": "BqIe8BoGkWw4"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encode primary, secondary and song type labels"
      ],
      "metadata": {
        "id": "8c2XWpEAFYvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def multi_hot_encode(labels, label_to_idx, num_classes):\n",
        "    \"\"\"Encodes multiple labels into a multi-hot vector\"\"\"\n",
        "    if not isinstance(labels, list):\n",
        "        labels = [labels]  # Ensure primary label is treated as list\n",
        "\n",
        "    # Get indices for all valid labels\n",
        "    indices = [label_to_idx[label] for label in labels if label in label_to_idx]\n",
        "\n",
        "    # Create multi-hot vector\n",
        "    encoded = torch.zeros(num_classes, dtype=torch.float32)\n",
        "    if indices:\n",
        "        encoded.scatter_(0, torch.tensor(indices), 1.0)\n",
        "    return encoded\n",
        "\n",
        "# Apply encoding to each column\n",
        "train_df['primary_label_one_hot'] = train_df['primary_label'].apply(\n",
        "    lambda x: multi_hot_encode(x, bird_species_to_label, len(unique_bird_species)))\n",
        "\n",
        "train_df['secondary_labels_one_hot'] = train_df['secondary_labels'].apply(\n",
        "    lambda x: multi_hot_encode(x, bird_species_to_label, len(unique_bird_species)))\n",
        "\n",
        "train_df['type_one_hot'] = train_df['type'].apply(\n",
        "    lambda x: multi_hot_encode(x, song_type_to_label, len(unique_song_types)))\n",
        "\n",
        "# Verify output shapes\n",
        "print(f\"Primary label shape: {train_df['primary_label_one_hot'][0].shape}\")\n",
        "print(f\"Secondary labels shape: {train_df['secondary_labels_one_hot'][0].shape}\")\n",
        "print(f\"Type shape: {train_df['type_one_hot'][0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oODxt7Y217VM",
        "outputId": "21bd962b-20fd-4bbd-9859-a485847a98e9"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primary label shape: torch.Size([10])\n",
            "Secondary labels shape: torch.Size([10])\n",
            "Type shape: torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make column for position\n",
        "---\n",
        "Format is (latitude, longitude)"
      ],
      "metadata": {
        "id": "VzMNDx_7KeQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['pos'] = list(zip(train_df['latitude'], train_df['longitude']))\n"
      ],
      "metadata": {
        "id": "Kws-orXgJYjS"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def load_spectrogram_h5(h5_path):\n",
        "    \"\"\"\n",
        "    Load spectrogram data from HDF5 file\n",
        "\n",
        "    Args:\n",
        "        h5_path: Path to .h5 file containing spectrogram\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: 2D spectrogram array\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If file doesn't exist\n",
        "        ValueError: If file is corrupt or invalid format\n",
        "    \"\"\"\n",
        "    h5_path = Path(h5_path)\n",
        "\n",
        "    if not h5_path.exists():\n",
        "        raise FileNotFoundError(f\"HDF5 file not found: {h5_path}\")\n",
        "\n",
        "    try:\n",
        "        with h5py.File(h5_path, 'r') as hf:\n",
        "            # Load spectrogram dataset\n",
        "            spectrogram = hf['spectrogram'][:]\n",
        "\n",
        "            # Basic validation\n",
        "            if not isinstance(spectrogram, np.ndarray):\n",
        "                raise ValueError(\"Spectrogram data is not a numpy array\")\n",
        "            if spectrogram.ndim != 2:\n",
        "                raise ValueError(f\"Expected 2D array, got {spectrogram.ndim}D\")\n",
        "            if np.isnan(spectrogram).any():\n",
        "                raise ValueError(\"Spectrogram contains NaN values\")\n",
        "\n",
        "            return spectrogram\n",
        "\n",
        "    except KeyError:\n",
        "        raise ValueError(\"HDF5 file missing 'spectrogram' dataset\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error loading HDF5 file: {str(e)}\")"
      ],
      "metadata": {
        "id": "P1CkIwZiicrP"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slice_spectrogram(spectrogram, sr=32000, chunk_seconds=5, overlap=0.5):\n",
        "    hop_length = 512\n",
        "    frames_per_chunk = int(chunk_seconds * sr / hop_length)  # 312 at 32kHz\n",
        "    hop_frames = int(frames_per_chunk * (1 - overlap))       # 156 at 50% overlap\n",
        "\n",
        "    n_frames = spectrogram.shape[1]\n",
        "    chunks = []\n",
        "\n",
        "    for start in range(0, n_frames - frames_per_chunk + 1, hop_frames):\n",
        "        end = start + frames_per_chunk\n",
        "        chunks.append(spectrogram[:, start:end])\n",
        "\n",
        "    #print(f\"Generated {len(chunks)} chunks from {n_frames * hop_length / sr:.2f}s audio\")\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "Z2jojirmh0-k"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def pad_spectrogram(mel, max_width):\n",
        "    if isinstance(mel, list):\n",
        "        mel = torch.tensor(mel, dtype=torch.float32)  # convert list to tensor\n",
        "    elif isinstance(mel, np.ndarray):\n",
        "        mel = torch.from_numpy(mel).float()           # if it's a NumPy array\n",
        "\n",
        "    # mel: [mel_bins, time]\n",
        "    if mel.shape[1] < max_width:\n",
        "        pad_width = max_width - mel.shape[1]\n",
        "        mel = F.pad(mel, (0, pad_width), mode='constant', value=0)\n",
        "    else:\n",
        "        mel = mel[:, :max_width]\n",
        "    return mel\n"
      ],
      "metadata": {
        "id": "e2EMfTS9PQHh"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_in_seconds = 15\n",
        "max_width = split_in_seconds * SAMPLE_RATE // HOP_LENGTH\n",
        "\n",
        "def change_extension_to_h5(filename):\n",
        "    \"\"\"Change file extension from .ogg to .h5\"\"\"\n",
        "    base = os.path.splitext(filename)[0]  # Get filename without extension\n",
        "    return base + '.h5'  # Add .h5 extension\n",
        "\n",
        "class BirdClefDataset(Dataset):\n",
        "    def __init__(self, df, target_dir, audio_transform=None, max_width=max_width):\n",
        "        self.df = df\n",
        "        self.target_dir = target_dir\n",
        "        self.audio_transform = audio_transform\n",
        "        self.max_width = max_width\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        return:\n",
        "          mel:       Spectogram chunks\n",
        "          primary:   Primary species label one-hot (only one)\n",
        "          secondary: Secondary species label one-hot (multiple)\n",
        "          type:      Song type onehot\n",
        "          pos:       Position of recording in (latitude, longitude)\n",
        "        \"\"\"\n",
        "        row = self.df.iloc[idx]\n",
        "        filename = row[\"filename\"]\n",
        "        species, filename = filename.split(\"/\")\n",
        "        filename = change_extension_to_h5(filename)\n",
        "\n",
        "        spectogram = os.path.join(self.target_dir, \"train_spectrograms\", species, filename)\n",
        "\n",
        "        # Load spectrogram\n",
        "        mel = load_spectrogram_h5(spectogram)\n",
        "        chunks = slice_spectrogram(mel)\n",
        "        if len(chunks) == 0:\n",
        "            chunk = pad_spectrogram(mel, self.max_width)\n",
        "            chunks = [chunk]\n",
        "\n",
        "        chunks = [pad_spectrogram(chunk, self.max_width) for chunk in chunks]        # Apply audio transform\n",
        "        chunks = torch.stack(chunks)  # Convert list → tensor: [N_chunks, mel_bins, time]\n",
        "\n",
        "        # labels\n",
        "\n",
        "        primary = row[\"primary_label_one_hot\"]\n",
        "        secondary = row[\"secondary_labels_one_hot\"]\n",
        "        song_type = row[\"type_one_hot\"]\n",
        "        pos = row[\"pos\"]\n",
        "\n",
        "        primary = primary.clone().detach()\n",
        "        song_type = song_type.clone().detach()\n",
        "        pos = torch.tensor(pos, dtype=torch.float32)\n",
        "\n",
        "        return chunks, primary, song_type, pos\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "val_dataset = BirdClefDataset(val_df, target_dir, max_width=max_width)\n",
        "train_dataset = BirdClefDataset(train_df, target_dir)"
      ],
      "metadata": {
        "id": "8F2-ICshN9lH"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_dataset[0]\n",
        "sample"
      ],
      "metadata": {
        "id": "c6zPM9wmp-7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae89697f-6040-4f4c-d26e-a5f6271cdeef"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-77.9510, -44.9678, -38.2330,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-77.0556, -44.9755, -34.8378,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-78.2797, -50.2456, -41.3816,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          ...,\n",
              "          [-80.0000, -60.6602, -54.0683,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-80.0000, -61.3213, -57.1995,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-80.0000, -67.3831, -56.4218,  ...,   0.0000,   0.0000,   0.0000]],\n",
              " \n",
              "         [[-52.3345, -48.9863, -44.5258,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-38.1167, -37.0272, -41.8713,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-38.4341, -35.1506, -32.8620,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          ...,\n",
              "          [-53.9844, -51.0757, -49.4870,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-56.3121, -59.5655, -58.4470,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-56.2035, -56.2659, -55.1153,  ...,   0.0000,   0.0000,   0.0000]],\n",
              " \n",
              "         [[-40.0869, -43.7006, -48.8756,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-38.0293, -39.8650, -35.3224,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-34.9582, -39.8977, -35.9754,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          ...,\n",
              "          [-51.6769, -49.6766, -51.3078,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-52.4965, -55.1173, -56.9028,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-52.2720, -52.1587, -52.2771,  ...,   0.0000,   0.0000,   0.0000]],\n",
              " \n",
              "         [[-45.6344, -35.6128, -33.8478,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-30.8616, -30.0402, -28.8109,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-29.5274, -29.1280, -29.1266,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          ...,\n",
              "          [-57.2171, -54.5079, -52.3242,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-53.3214, -54.1315, -52.9820,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-51.6108, -52.1763, -51.2478,  ...,   0.0000,   0.0000,   0.0000]],\n",
              " \n",
              "         [[-26.9446, -25.8932, -31.8598,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-15.4003, -20.2363, -25.1633,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-16.6664, -26.7846, -24.3935,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          ...,\n",
              "          [-45.3412, -44.0159, -43.7698,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-42.2944, -41.6254, -41.7453,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [-42.7156, -42.2734, -44.5511,  ...,   0.0000,   0.0000,   0.0000]]]),\n",
              " tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
              " tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
              " tensor([ -0.6410, -76.4610]))"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create batches where all the primary labels are the same"
      ],
      "metadata": {
        "id": "lkfmjTYcHk8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "species_to_indices = {}\n",
        "for idx, row in train_df.iterrows():\n",
        "    species = row['primary_label']\n",
        "    if species not in species_to_indices:\n",
        "        species_to_indices[species] = []\n",
        "    species_to_indices[species].append(idx)"
      ],
      "metadata": {
        "id": "3Jzr329CSFs3"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def simple_collate_fn(batch):\n",
        "    chunk_lists, primary_list, type_list, pos_list = zip(*batch)\n",
        "\n",
        "    max_chunks = 4  # reduce memory usage\n",
        "    padded_chunks = []\n",
        "    for chunks in chunk_lists:\n",
        "        chunks = chunks[:max_chunks]\n",
        "        pad_amt = max_chunks - chunks.shape[0]\n",
        "        if pad_amt > 0:\n",
        "            pad = torch.zeros((pad_amt, *chunks.shape[1:]))\n",
        "            chunks = torch.cat([chunks, pad], dim=0)\n",
        "        padded_chunks.append(chunks)\n",
        "\n",
        "    mel = torch.stack(padded_chunks).unsqueeze(2)  # [B, max_chunks, 1, mel_bins, time]\n",
        "    primary = torch.stack(primary_list)\n",
        "    song_type = torch.stack(type_list)\n",
        "    pos = torch.stack(pos_list)\n",
        "\n",
        "    return mel, primary, song_type, pos\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,         # small batch\n",
        "    shuffle=True,\n",
        "    collate_fn=simple_collate_fn,\n",
        "    num_workers=0,        # critical for low RAM\n",
        "    pin_memory=False\n",
        ")\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=simple_collate_fn, num_workers=0)"
      ],
      "metadata": {
        "id": "xBWI_hndueX-"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "7mQF1-uZ7jm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(Head, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "class JPM(nn.Module):\n",
        "    def __init__(self, in_dim, s_dim, t_dim):\n",
        "        super(JPM, self).__init__()\n",
        "        z_dim = 256\n",
        "        c_dim = 2  # latitude and longitude\n",
        "\n",
        "        # Shared convolutional backbone (Net_Z(X))\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, z_dim)  # assuming input is pre-scaled so output size is 128\n",
        "        )\n",
        "\n",
        "        # Independent predictions\n",
        "        self.indep_s = Head(z_dim, s_dim)  # species (classification)\n",
        "        self.indep_t = Head(z_dim, t_dim)  # song type (classification)\n",
        "        self.indep_c = Head(z_dim, c_dim)  # coordinates (regression)\n",
        "\n",
        "        # Joint predictions 1\n",
        "        self.joint_ts = Head(z_dim + t_dim, s_dim)\n",
        "        self.joint_cs = Head(z_dim + c_dim, s_dim)\n",
        "\n",
        "        self.joint_st = Head(z_dim + s_dim, t_dim)\n",
        "        self.joint_ct = Head(z_dim + c_dim, t_dim)\n",
        "\n",
        "        self.joint_sc = Head(z_dim + s_dim, c_dim)\n",
        "        self.joint_tc = Head(z_dim + t_dim, c_dim)\n",
        "\n",
        "        # Joint predictions 2\n",
        "        self.joint_tcs = Head(z_dim + t_dim + c_dim, s_dim)\n",
        "        self.joint_cts = Head(z_dim + c_dim + t_dim, s_dim)\n",
        "\n",
        "        self.joint_sct = Head(z_dim + s_dim + c_dim, t_dim)\n",
        "        self.joint_cst = Head(z_dim + c_dim + s_dim, t_dim)\n",
        "\n",
        "        self.joint_stc = Head(z_dim + s_dim + t_dim, c_dim)\n",
        "        self.joint_tsc = Head(z_dim + t_dim + s_dim, c_dim)\n",
        "\n",
        "    def forward(self, x, y_s=None, y_t=None, y_c=None):\n",
        "        z = self.backbone(x)\n",
        "\n",
        "        # Independent predictions\n",
        "        s = self.indep_s(z)\n",
        "        t = self.indep_t(z)\n",
        "        c = self.indep_c(z)\n",
        "\n",
        "        # Inference case\n",
        "        if y_s is None:\n",
        "            y_s = s.detach()\n",
        "            y_t = t.detach()\n",
        "            y_c = c.detach()\n",
        "\n",
        "        # y_s = F.one_hot(y_s.argmax(dim=-1), num_classes=s.shape[-1]).float()\n",
        "        # y_t = F.one_hot(y_t.argmax(dim=-1), num_classes=t.shape[-1]).float()\n",
        "\n",
        "        y_s = F.one_hot(y_s, num_classes=s.shape[-1]).float()\n",
        "        y_t = F.one_hot(y_t, num_classes=t.shape[-1]).float()\n",
        "\n",
        "        # Note: y_c is already continuous, do not one-hot\n",
        "\n",
        "        zs = torch.cat([z, y_s], dim=1)\n",
        "        zt = torch.cat([z, y_t], dim=1)\n",
        "        zc = torch.cat([z, y_c], dim=1)\n",
        "\n",
        "        # Joint 1\n",
        "        ts = self.joint_ts(zt)\n",
        "        cs = self.joint_cs(zc)\n",
        "        st = self.joint_st(zs)\n",
        "        ct = self.joint_ct(zc)\n",
        "        sc = self.joint_sc(zs)\n",
        "        tc = self.joint_tc(zt)\n",
        "\n",
        "        # For inference: optional second-stage values\n",
        "        if y_s is None:\n",
        "            y_s2 = ts.detach()\n",
        "            y_s3 = cs.detach()\n",
        "            y_t2 = st.detach()\n",
        "            y_t3 = ct.detach()\n",
        "            y_c2 = sc.detach()\n",
        "            y_c3 = tc.detach()\n",
        "        else:\n",
        "            y_s2 = ts\n",
        "            y_s3 = cs\n",
        "            y_t2 = st\n",
        "            y_t3 = ct\n",
        "            y_c2 = sc\n",
        "            y_c3 = tc\n",
        "\n",
        "        zts = torch.cat([z, y_t, y_s2], dim=1)\n",
        "        zcs = torch.cat([z, y_c, y_s3], dim=1)\n",
        "        zst = torch.cat([z, y_s, y_t2], dim=1)\n",
        "        zct = torch.cat([z, y_c, y_t3], dim=1)\n",
        "        zsc = torch.cat([z, y_s, y_c2], dim=1)\n",
        "        ztc = torch.cat([z, y_t, y_c3], dim=1)\n",
        "\n",
        "        # Joint 2\n",
        "        tcs = self.joint_tcs(zct)\n",
        "        cts = self.joint_cts(zct)\n",
        "        sct = self.joint_sct(zsc)\n",
        "        cst = self.joint_cst(zsc)\n",
        "        stc = self.joint_stc(zst)\n",
        "        tsc = self.joint_tsc(zst)\n",
        "\n",
        "        s_logits = [s, ts, cs, tcs, cts]\n",
        "        t_logits = [t, st, ct, sct, cst]\n",
        "        c_preds = [c, sc, tc, stc, tsc]  # Continuous [lat, lon]\n",
        "\n",
        "        return s_logits, t_logits, c_preds\n"
      ],
      "metadata": {
        "id": "aRz2t0Bh7mE2"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    y_true_species = []\n",
        "    y_pred_species = []\n",
        "    y_true_type = []\n",
        "    y_pred_type = []\n",
        "    mse_total = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mel, primary, song_type, pos in val_loader:\n",
        "            B, N_chunks, C, H, W = mel.shape\n",
        "            mel = mel.view(B * N_chunks, C, H, W).to(device)\n",
        "\n",
        "            # Get labels per recording\n",
        "            y_s = primary.argmax(dim=1).to(device)\n",
        "            y_t = song_type.argmax(dim=1).to(device)\n",
        "            y_c = pos.to(device)\n",
        "\n",
        "            # Repeat labels per chunk for model input\n",
        "            y_s_repeated = y_s.repeat_interleave(N_chunks)\n",
        "            y_t_repeated = y_t.repeat_interleave(N_chunks)\n",
        "            y_c_repeated = y_c.repeat_interleave(N_chunks, dim=0)\n",
        "\n",
        "            # Forward pass\n",
        "            s_logits, t_logits, c_preds = model(mel, y_s_repeated, y_t_repeated, y_c_repeated)\n",
        "\n",
        "            # Aggregate logits across chunks\n",
        "            s_main = s_logits[0].view(B, N_chunks, -1).mean(dim=1)\n",
        "            t_main = t_logits[0].view(B, N_chunks, -1).mean(dim=1)\n",
        "            c_main = c_preds[0].view(B, N_chunks, -1).mean(dim=1)\n",
        "\n",
        "            # Store predictions and labels for metrics\n",
        "            y_true_species.extend(y_s.cpu().numpy())\n",
        "            y_pred_species.extend(s_main.argmax(dim=1).cpu().numpy())\n",
        "            y_true_type.extend(y_t.cpu().numpy())\n",
        "            y_pred_type.extend(t_main.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "            mse_total += F.mse_loss(c_main, y_c, reduction='sum').item()\n",
        "            total_samples += B\n",
        "\n",
        "    precision_species = precision_score(y_true_species, y_pred_species, average='macro', zero_division=0)\n",
        "    precision_type = precision_score(y_true_type, y_pred_type, average='macro', zero_division=0)\n",
        "    mse_coord = mse_total / total_samples\n",
        "\n",
        "    print(f\"🔍 Val Precision — Species: {precision_species:.3f} | Type: {precision_type:.3f} | Coord MSE: {mse_coord:.4f}\")\n",
        "    model.train()\n",
        "    return (\n",
        "        precision_species, precision_type, mse_coord,\n",
        "        y_true_species, y_pred_species,\n",
        "        y_true_type, y_pred_type\n",
        "    )\n"
      ],
      "metadata": {
        "id": "MFT4L9cP2zhy"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_precisions_species = []\n",
        "val_precisions_type = []\n",
        "val_mse_coords = []\n"
      ],
      "metadata": {
        "id": "LgMNoz2bKkxl"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model\n",
        "model = JPM(\n",
        "    in_dim=1,\n",
        "    s_dim=len(bird_species_to_label),\n",
        "    t_dim=len(song_type_to_label),\n",
        ").to(device)\n",
        "\n",
        "# Loss functions\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "\n",
        "    for mel, primary, song_type, pos in loop:\n",
        "        B, N_chunks, C, H, W = mel.shape\n",
        "        mel = mel.view(B * N_chunks, C, H, W).to(device)\n",
        "\n",
        "        y_s = torch.argmax(primary, dim=1).to(device)\n",
        "        y_t = torch.argmax(song_type, dim=1).to(device)\n",
        "        y_c = pos.to(device)\n",
        "\n",
        "        y_s_repeated = y_s.repeat_interleave(N_chunks)\n",
        "        y_t_repeated = y_t.repeat_interleave(N_chunks)\n",
        "        y_c_repeated = y_c.repeat_interleave(N_chunks, dim=0)\n",
        "\n",
        "        s_logits, t_logits, c_preds = model(mel, y_s_repeated, y_t_repeated, y_c_repeated)\n",
        "\n",
        "        s_logit_main = s_logits[0].view(B, N_chunks, -1).mean(dim=1)\n",
        "        t_logit_main = t_logits[0].view(B, N_chunks, -1).mean(dim=1)\n",
        "        c_pred_main = c_preds[0].view(B, N_chunks, -1).mean(dim=1)\n",
        "\n",
        "        loss_s = ce_loss(s_logit_main, y_s)\n",
        "        loss_t = ce_loss(t_logit_main, y_t)\n",
        "        loss_c = mse_loss(c_pred_main, y_c)\n",
        "\n",
        "        loss = 1.5 * loss_s + loss_t + 0.5 * loss_c\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} — Avg Loss: {avg_loss:.4f}\")\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # Get all evaluation results\n",
        "    precision_s, precision_t, mse_c, y_true_s, y_pred_s, y_true_t, y_pred_t = evaluate(model, val_loader, device)\n",
        "    val_precisions_species.append(precision_s)\n",
        "    val_precisions_type.append(precision_t)\n",
        "    val_mse_coords.append(mse_c)\n"
      ],
      "metadata": {
        "id": "N6nzKpVwV33B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfac542-98f1-453e-8d06-438623f82774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  24%|██▍       | 109/450 [03:36<12:01,  2.12s/it, loss=297]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Plot training metrics\n",
        "epochs = range(1, num_epochs + 1)\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Training loss\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(epochs, train_losses, marker='o')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "# Precision\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(epochs, val_precisions_species, marker='o', label='Species')\n",
        "plt.plot(epochs, val_precisions_type, marker='x', label='Song Type')\n",
        "plt.title(\"Validation Precision\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.legend()\n",
        "\n",
        "# Coord MSE\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(epochs, val_mse_coords, marker='o', color='red')\n",
        "plt.title(\"Coordinate MSE\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix — Species\n",
        "species_classes = list(bird_species_to_label.keys())\n",
        "species_labels = list(range(len(species_classes)))\n",
        "\n",
        "cm_species = confusion_matrix(y_true_s, y_pred_s, labels=species_labels)\n",
        "disp_species = ConfusionMatrixDisplay(confusion_matrix=cm_species, display_labels=species_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp_species.plot(xticks_rotation=45, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix — Species\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix — Song Type\n",
        "type_classes = list(song_type_to_label.keys())\n",
        "type_labels = list(range(len(type_classes)))\n",
        "\n",
        "cm_type = confusion_matrix(y_true_t, y_pred_t, labels=type_labels)\n",
        "disp_type = ConfusionMatrixDisplay(confusion_matrix=cm_type, display_labels=type_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp_type.plot(xticks_rotation=45, cmap='Oranges')\n",
        "plt.title(\"Confusion Matrix — Song Type\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tf8EpfM4Ktzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0P9IL3PmT3OF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}